
%\author[Niklas Brunn]{Nix}


\beamertemplatenavigationsymbolsempty{}

\logo{\includegraphics[height=1cm]{Bilder/logo}}

\section{Formulation of the optimization problem}

  \subsection{Optimization method}
  \begin{frame}
  \frametitle{Optimization method}
  \begin{itemize}
  	\item Newton-type optimization, $k$-th step: 
  	$$\theta_{k+1} = \theta_{k} - \mathrm{B}_{k}^{-1}\cdot\nabla_{\theta_{k}} f_{D},$$%\theta_{k} - \mathrm{B}_{k}^{-1}\cdot\left(\frac{\partial}{\partial \theta}\:f_{D}\bigg\vert_{\theta = \theta_{k}}\right)^{\mathrm{T}} =$$
  	\pause
  	\item[] the Hessian approximation is  a damped version of the Generalised Gauss Newton matrix 
  	$$\mathrm{B} := \left(\frac{1}{N}\sum_{(x, y)\in\mathrm{D}}^{}\mathrm{J}_{\mathrm{R}_{x}(\theta)}^{\mathrm{T}}\:\mathrm{H}_{\mathrm{L_{y}\circ\:\text{act}}}\:\mathrm{J}_{\mathrm{R}_{x}(\theta)}\right) + \lambda\:\mathrm{I}^{d\times d} $$
  	\item Equivalent formulation of the $k$-th step:
  	$$B_{k}\cdot (\theta_{k+1} - \theta_{k}) = -\nabla_{\theta_{k}}f_{D}$$
    %\pause
  	%\item updates with (preconditioned) Conjugate Gradient method using efficient matrix vector products with the $\mathcal{R}$-operator
  \end{itemize}
\end{frame}

\subsection{Implementation details and computational efficiency}
\begin{frame}
\frametitle{Implementation details and computational efficiency}
\begin{itemize}
	\item[] $$B_{k}\cdot \Delta\theta = -\nabla_{\theta_{k}}f_{D}$$
	\item We approximate the solution of this linear system with the (preconditioned) Conjugate gradient method
	%$$\mathcal{R}_{v}(F(x)) := \frac{\partial F(x + t\cdot v)}{\partial t}\bigg\vert_{t = 0} = \mathrm{J}_{F}\: v $$
	\pause
	\item Therefore we use efficient matrix-vector-products for extra computational efficiency using Forward- and Backward-AD
\end{itemize}
\end{frame}
  
